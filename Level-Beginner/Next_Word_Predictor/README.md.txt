# AI-Powered Spell Checker & Next Word Predictor

An intelligent text prediction system combining:

- âœ… 3-word LSTM Language Model (Primary)
- âœ… 2-word LSTM Language Model (Secondary)
- âœ… Trigram Statistical Model (Fallback)
- âœ… Advanced Word Split + Spell Correction Engine

This system performs:

- ğŸ”¤ Automatic spell correction
- ğŸ” Detection of accidentally joined words (e.g., `loveinthe` â†’ `love in the`)
- ğŸ§  Context-aware next word prediction
- âš¡ Real-time prediction API using Flask

---

## ğŸš€ Project Architecture

Pipeline:

1. Preprocessing (lowercase, punctuation removal)
2. Word Split Correction (DP-based segmentation + spell correction)
3. Spell Checking
4. Multi-model Prediction:
   - 3-word LSTM (highest priority)
   - Trigram Model
   - 2-word LSTM (controlled influence)
5. Weighted Prediction Merge

---

## ğŸ§  Models Used

| Model | Context Size | Role |
|--------|--------------|------|
| LSTM 3-word | 3 previous words | Primary predictor |
| LSTM 2-word | 2 previous words | Secondary |
| Trigram | 2-word statistical | Baseline fallback |

---

## ğŸ“‚ Project Structure
SpellChecker/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ lstm_3word_800k_e75_final.keras
â”‚ â”œâ”€â”€ nextword_lstm_200k.keras
â”‚ â”œâ”€â”€ tokenizer.pkl
â”‚ â”œâ”€â”€ trigram.pkl
â”‚ â”œâ”€â”€ vocab.pkl
â”‚
â”œâ”€â”€ templates/
â”‚ â””â”€â”€ index.html
â”‚
â””â”€â”€ static/
â”œâ”€â”€ style.css
â””â”€â”€ script.js


---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone <your_repo_url>
cd SpellChecker

2ï¸âƒ£ Create virtual environment (recommended)
python -m venv venv
venv\Scripts\activate     # Windows

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Run the application
python app.py


Then open:

http://127.0.0.1:5000

ğŸ“Š Model Details

Training dataset: Movie dialogue corpus (~4.5M tokens)

3-word LSTM trained on ~800k tokens

75+ epochs training

Custom tokenizer and vocabulary

Dynamic temperature sampling for better diversity

ğŸ“Œ Notes for Internship Review

The 2-word LSTM training logic was removed from the final notebook for clarity.

Only the 3-word LSTM training implementation is retained in the training version.

The deployed application uses pre-trained models stored in /models.

Training code is intentionally separated from deployment code to avoid accidental retraining and to follow production best practices.

ğŸ§ª Example Predictions

Input:

What are you giong


Output:

doing
going
talking


Input:

I loveintheyou


Corrected:

i love in the you

ğŸ›  Technologies Used

Python

TensorFlow / Keras

Flask

NumPy

pyspellchecker

HTML / CSS / JavaScript

ğŸ”® Future Improvements

Transformer-based prediction model

Beam search decoding

Real-time incremental learning

REST API deployment