{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7451f98-a8a2-4391-af67-48559d8f51d6",
   "metadata": {},
   "source": [
    "# Dataset Import/Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f9b796-9fb5-4195-b432-2d6a741534d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9a0d5c-e263-48fc-b385-c3b3b7a7ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\tWell, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\tNot the hacking and gagging and spitting part.  Please.\n",
      "Not the hacking and gagging and spitting part.  Please.\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "You're asking me out.  That's so cut\n"
     ]
    }
   ],
   "source": [
    "with open(\"movie_lines.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25989b03-46e3-41bd-aab6-715a7081fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24654712\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e606b9bf-c928-4c72-8001-e0176072f4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you. Not the hacking and gagging and spitting part.  Please.\n",
      "Not the hacking and gagging and spitting part.  Please. Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "You're asking me out.  That's so cut\n"
     ]
    }
   ],
   "source": [
    "text = text.replace(\"\\t\", \" \")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94dc439d-898d-4ee1-bae9-a9317a1f7d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24654712\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18078b41-d095-43db-9c1a-f30abcbed14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can we make this quick?  roxanne korrine and andrew barrett are having an incredibly horrendous public break- up on the quad.  again. well, i thought we'd start with pronunciation, if that's okay with you.\n",
      "well, i thought we'd start with pronunciation, if that's okay with you. not the hacking and gagging and spitting part.  please.\n",
      "not the hacking and gagging and spitting part.  please. okay... then how 'bout we try out some french cuisine.  saturday?  night?\n",
      "you're asking me out.  that's so cut\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5120a945-580f-400c-b521-69d838da2c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again well i thought wed start with pronunciation if thats okay with you\n",
      "well i thought wed start with pronunciation if thats okay with you not the hacking and gagging and spitting part  please\n",
      "not the hacking and gagging and spitting part  please okay then how bout we try out some french cuisine  saturday  night\n",
      "youre asking me out  thats so cute whats your name again forg\n"
     ]
    }
   ],
   "source": [
    "for ch in string.punctuation:\n",
    "    text = text.replace(ch, \"\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f96828-f122-4ba5-a472-ab3f1ec09ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'we', 'make', 'this', 'quick', 'roxanne', 'korrine', 'and', 'andrew', 'barrett', 'are', 'having', 'an', 'incredibly', 'horrendous', 'public', 'break', 'up', 'on', 'the', 'quad', 'again', 'well', 'i', 'thought', 'wed', 'start', 'with', 'pronunciation', 'if', 'thats', 'okay', 'with', 'you', 'well', 'i', 'thought', 'wed', 'start', 'with', 'pronunciation', 'if', 'thats', 'okay', 'with', 'you', 'not', 'the', 'hacking', 'and']\n",
      "Total Words:  4569409\n"
     ]
    }
   ],
   "source": [
    "tokens = text.split()\n",
    "print(tokens[:50])\n",
    "print(\"Total Words: \", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f2998c8-e173-4cce-908e-f07b155cf9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tokens = tokens[:800000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b33466-9858-40ac-9822-3a39d2ca1966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 66276\n"
     ]
    }
   ],
   "source": [
    "vocab = set(tokens)\n",
    "print(\"Vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f5780-144d-42d8-a7a7-a74ad258c169",
   "metadata": {},
   "source": [
    "# Bigram Model Creation and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92c56692-9215-458a-95d8-bfc7e3263fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4102f42c-2f44-4fcd-8d44-50e0d13f03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokens) - 1):\n",
    "    current_word = tokens[i]\n",
    "    next_word = tokens[i + 1]\n",
    "    bigram_model[current_word].append(next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99d8a176-3a25-4ac9-8427-abf5cbc7a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(word, k=3):\n",
    "    if word not in bigram_model:\n",
    "        return \"No Suggestion\"\n",
    "    next_words = bigram_model[word]\n",
    "    freq = Counter(next_words)\n",
    "    suggest = [w for w, _ in freq.most_common(k)]\n",
    "    return suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2252135-844d-42d4-a0a9-e2e8bd8f6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_sentence(sentence, k=3):\n",
    "    sentence = sentence.lower()\n",
    "    for ch in string.punctuation:\n",
    "        sentence = sentence.replace(ch, \"\")\n",
    "    words = sentence.split()\n",
    "    if len(words) == 0:\n",
    "        return \"No Input\"\n",
    "    last_word = words[-1]\n",
    "    return predict_next_word(last_word, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c4a67b7-8a9f-408b-a7aa-f0859064dd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'god', 'me']\n",
      "['you', 'we', 'the']\n",
      "['you', 'with', 'to']\n"
     ]
    }
   ],
   "source": [
    "print(predict_from_sentence(\"thank\"))\n",
    "print(predict_from_sentence(\"how are\"))\n",
    "print(predict_from_sentence(\"i love\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95cc2572-10cc-4d58-9325-99a04514285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram model saved.\n"
     ]
    }
   ],
   "source": [
    "# Save Bigram Model\n",
    "with open(\"bigram.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bigram_model, f)\n",
    "print(\"Bigram model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79501bfb-7ae6-4f2e-ba22-19cd6ad16cdb",
   "metadata": {},
   "source": [
    "# Trigram Model Creation and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f763aea-3fa0-4316-a74b-ed87cefa8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d8cc9c2-af3f-4e36-a614-127329af0cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokens) - 2):\n",
    "    w1 = tokens[i]\n",
    "    w2 = tokens[i + 1]\n",
    "    w3 = tokens[i + 2]\n",
    "    trigram_model[(w1, w2)].append(w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41bd4409-427e-44a9-8429-ae9ddf455af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_trigram(sentence, k=3):\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    for ch in string.punctuation:\n",
    "        sentence = sentence.replace(ch, \"\")\n",
    "\n",
    "    words = sentence.split()\n",
    "\n",
    "    if len(words) < 2:\n",
    "        return [\"Need more words\"]\n",
    "\n",
    "    # Spell Correction Step\n",
    "    corrected_words = []\n",
    "    for w in words:\n",
    "        if w in vocab:\n",
    "            corrected_words.append(w)\n",
    "        else:\n",
    "            corrected_words.append(spell.correction(w))\n",
    "\n",
    "    # Take last two words after correction\n",
    "    key = (corrected_words[-2], corrected_words[-1])\n",
    "\n",
    "    if key not in trigram_model:\n",
    "        return predict_next_word(corrected_words[-1])\n",
    "\n",
    "    freq = Counter(trigram_model[key])\n",
    "    suggestions = [w for w, _ in freq.most_common(k)]\n",
    "\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6625910f-e840-4556-9b54-45b279bbcbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'it', 'her']\n",
      "['you', 'we', 'things']\n",
      "['be', 'do', 'have']\n"
     ]
    }
   ],
   "source": [
    "print(predict_trigram(\"i love\"))\n",
    "print(predict_trigram(\"how are\"))\n",
    "print(predict_trigram(\"going to\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30542eb0-0888-4d87-b0fc-0888cb39030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trigram(sample_size=20000):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(sample_size):\n",
    "        w1, w2, actual = tokens[i], tokens[i + 1], tokens[i + 2]\n",
    "        prediction = predict_trigram(f\"{w1} {w2}\", k=1)\n",
    "        if prediction[0] == actual:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba4bbb62-8b02-4bfc-a720-91e361ecdfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4065\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate_trigram()\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92130459-8866-447c-bd53-b78ef6043685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'it', 'her']\n"
     ]
    }
   ],
   "source": [
    "print(predict_trigram(\"I lvoe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84961671-a3b5-4ad3-9f38-236b32c82897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram model saved.\n"
     ]
    }
   ],
   "source": [
    "# Save Trigram Model\n",
    "with open(\"trigram.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trigram_model, f)\n",
    "print(\"Trigram model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f117df8-5a70-44c1-a746-a8bfd2bc002c",
   "metadata": {},
   "source": [
    "# LSTM_2 (Predict Next Word Using 2 Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9ae4d4a-69e5-4e2b-9535-de1c7b2fa5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b6d2d64-0a06-4ea1-919f-07f8b1db7911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 23217\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer if exists, else create new\n",
    "try:\n",
    "    with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(small_tokens)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "index_word = {v: k for k, v in word_index.items()}\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3e5a947-05c7-4423-a11f-a4dd2172100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2 shape: (799998, 2)\n"
     ]
    }
   ],
   "source": [
    "# Build X, y using 2-word input windows\n",
    "X2 = []\n",
    "y2 = []\n",
    "\n",
    "for i in range(len(small_tokens) - 2):\n",
    "    w1 = small_tokens[i]\n",
    "    w2 = small_tokens[i + 1]\n",
    "    w3 = small_tokens[i + 2]\n",
    "    if w1 in word_index and w2 in word_index and w3 in word_index:\n",
    "        X2.append([word_index[w1], word_index[w2]])\n",
    "        y2.append(word_index[w3])\n",
    "\n",
    "X2 = np.array(X2)\n",
    "y2 = np.array(y2)\n",
    "print(\"X2 shape:\", X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f660123-8e2e-45c2-9160-643d65c939ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build LSTM_2 model (2 input words)\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(input_dim=vocab_size, output_dim=50))\n",
    "model_2.add(LSTM(100))\n",
    "model_2.add(Dense(vocab_size, activation='softmax'))\n",
    "model_2.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb30293e-f08a-4714-93de-c863c683cfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m326/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 237ms/step - loss: 8.2115"
     ]
    }
   ],
   "source": [
    "X2_train = X2[:800000]\n",
    "y2_train = y2[:800000]\n",
    "\n",
    "history_2 = model_2.fit(\n",
    "    X2_train,\n",
    "    y2_train,\n",
    "    epochs=5,\n",
    "    batch_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c29265-fc74-46f6-a4e0-89c67b4be709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lstm_2(sentence, k=3):\n",
    "    sentence = sentence.lower()\n",
    "    for ch in string.punctuation:\n",
    "        sentence = sentence.replace(ch, \"\")\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return \"Need at least 2 words\"\n",
    "    w1, w2 = words[-2], words[-1]\n",
    "    if w1 not in word_index or w2 not in word_index:\n",
    "        return \"Unknown word\"\n",
    "    x = np.array([[word_index[w1], word_index[w2]]])\n",
    "    prediction = model_2.predict(x, verbose=0)\n",
    "    top_ids = np.argsort(prediction[0])[-k:][::-1]\n",
    "    return [index_word.get(i, \"Unknown\") for i in top_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e56ed3-ed48-42b7-9c41-f4c4fe6aa0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_lstm_2(\"i love\"))\n",
    "print(predict_lstm_2(\"how are\"))\n",
    "print(predict_lstm_2(\"where are\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d603267d-9774-4950-bb61-0b5dbe9d541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LSTM_2 model and tokenizer\n",
    "model_2.save(\"lstm_2word.keras\")\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"LSTM_2 model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0323f7-a6ff-4a87-88e1-6e4cd42fcdfb",
   "metadata": {},
   "source": [
    "# LSTM_3 (Predict Next Word Using 3 Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d52864-8cf4-4702-8ed0-62516e0b3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build X, y using 3-word input windows\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(small_tokens) - 3):\n",
    "    w1 = small_tokens[i]\n",
    "    w2 = small_tokens[i + 1]\n",
    "    w3 = small_tokens[i + 2]\n",
    "    w4 = small_tokens[i + 3]\n",
    "    if w1 in word_index and w2 in word_index and w3 in word_index and w4 in word_index:\n",
    "        X.append([word_index[w1], word_index[w2], word_index[w3]])\n",
    "        y.append(word_index[w4])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"X shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0605a9-e7b1-4c3d-9d95-c1739b6f0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM_3 model (3 input words)\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=50))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c877f0-14b0-463e-9075-a9835dd00c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:800000]\n",
    "y_train = y[:800000]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35122c0a-bcd9-45f6-8b0c-aecf21995685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lstm_3(sentence, k=3):\n",
    "    sentence = sentence.lower()\n",
    "    for ch in string.punctuation:\n",
    "        sentence = sentence.replace(ch, \"\")\n",
    "    words = sentence.split()\n",
    "    if len(words) < 3:\n",
    "        return \"Need at least 3 words\"\n",
    "    w1, w2, w3 = words[-3], words[-2], words[-1]\n",
    "    if w1 not in word_index or w2 not in word_index or w3 not in word_index:\n",
    "        return \"Unknown word\"\n",
    "    x = np.array([[word_index[w1], word_index[w2], word_index[w3]]])\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    top_ids = np.argsort(prediction[0])[-k:][::-1]\n",
    "    return [index_word.get(i, \"Unknown\") for i in top_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5ccce-a6a8-4b7a-a591-9b20240bbee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_lstm_3(\"i love how\"))\n",
    "print(predict_lstm_3(\"how are we\"))\n",
    "print(predict_lstm_3(\"where are you going\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36898d-87ea-4a39-9f58-eb58d053ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LSTM_3 model and tokenizer\n",
    "model.save(\"lstm_3word.keras\")\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"LSTM_3 model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
